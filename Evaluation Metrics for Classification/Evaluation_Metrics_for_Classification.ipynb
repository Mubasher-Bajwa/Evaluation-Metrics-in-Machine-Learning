{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Classification Problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us consider cancer data set and try to understand the metrics based on the classifier built for cancer prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  mean radius  mean texture  mean perimeter  mean area  \\\n",
       "0           0        17.99         10.38          122.80     1001.0   \n",
       "1           1        20.57         17.77          132.90     1326.0   \n",
       "2           2        19.69         21.25          130.00     1203.0   \n",
       "3           3        11.42         20.38           77.58      386.1   \n",
       "4           4        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   mean symmetry  ...  worst texture  worst perimeter  worst area  \\\n",
       "0         0.2419  ...          17.33           184.60      2019.0   \n",
       "1         0.1812  ...          23.41           158.80      1956.0   \n",
       "2         0.2069  ...          25.53           152.50      1709.0   \n",
       "3         0.2597  ...          26.50            98.87       567.7   \n",
       "4         0.1809  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing dataset\n",
    "df = pd.read_csv('cancer.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating independent and dependent variables\n",
    "X = df.iloc[:,:-1]\n",
    "y= df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=18)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a binary classifier using Random Forest Algorithm with default hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=18)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here X_test, y_test are the test data points\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "#### The accuracy of a classifier is calculated as the ratio of the total number of correctly predicted samples by the total number of samples.\n",
    "\n",
    "#### Accuracy metric can be used to evaluate the classifier when the data set is a balanced data set. Accuracy metric should not be used when the data set is imbalanced. Let us consider a data set with two target classes containing 100 samples out of which 95 samples belong to class 1 and 5 samples belong to class 2. When we try to build a classifier for the above data set, the classifier will be biased to class 1 and will result is predicting all the samples as class 1 samples. This will result in an accuracy of 95%, which is false. To avoid this mistake accuracy metric should be only used balanced data set.\n",
    "\n",
    "#### Now let us look into the code to get the accuracy of a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_Score: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy_Score: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "#### A confusion matrix is an N dimensional square matrix, where N represents total number of target classes or categories. Confusion matrix can be used to evaluate a classifier whenever the data set is imbalanced.\n",
    "\n",
    "#### Now let us look into the code to generate and plot the confusion matrix for our cancer classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39  2]\n",
      " [ 3 70]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are four important terms in a confusion matrix\n",
    "\n",
    "#### True Positives (TP): These are the cases where the predicted “Yes” actually belonged to class “Yes”.\n",
    "\n",
    "#### True Negatives (TN): These are the cases where the predicted “No” actually belonged to class “No”.\n",
    "\n",
    "#### False Positives (FP): These are the cases where the predicted “Yes” actually belonged to class “No”.\n",
    "\n",
    "#### False Negatives (FN): These are the cases where the predicted “No” actually belonged to class “Yes”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displaying the confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, cmap='Spectral')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above confusion matrix:\n",
    "\n",
    "True Positives (TP): 70\n",
    "False positives (FP): 2\n",
    "True Negatives (TN): 39\n",
    "False Negatives (FN): 3\n",
    "\n",
    "#### The accuracy of the classifier can be calculated from the confusion using the below formula:\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "\n",
    "The accuracy of our classifier is: (70+39) / (70+39+2+3) = 0.9561 = 95.61%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision (or Positive Predictive Value)\n",
    "#### Precision is the ratio of true positives (TP) by the sum of true positives (TP) and false positives (FP).\n",
    "\n",
    "#### Let us consider a data set with two target classes (say positive and negative) then precision tells us, out of total predicted positive values how many were actually positive. Precision should be used based on the use case. Take an example use case of spam detection. If our model detects a mail as spam which was not actually a spam mail then the user might miss an important mail i.e. here false positives should be reduced. So, in this use case we need to use precision as a metric to measure the quality of our classifier.\n",
    "\n",
    "#### Now let us look into the code to calculate the precision score for our cancer classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_Score: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision_Score: {precision}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall (or Sensitivity or True Positive Rate)\n",
    "#### Recall is the ratio of true positives (TP) by the sum of true positives (TP) and false negatives (FN).\n",
    "\n",
    "#### Let us consider a data set with two target classes (say positive and negative) then recall tells us, out of total actual negative values how many did our classifier predict negatively. Similar to precision, recall should also be used based on the use case. Take an example use case of cancer prediction. Consider a person who is actually having cancer but was predicted as a non-cancer patient by our classifier which can lead to mistreatment of the person i.e. here false negatives should be reduced. So, in this case, we need to use recall as a metric to measure the quality of our classifier.\n",
    "\n",
    "#### Now let us look into the code to calculate the recall score for our cancer classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall_Score: 0.958904109589041\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f'Recall_Score: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score\n",
    "#### F1 score should be used when both precision and recall are important for the use case. F1 score is the harmonic mean of precision and recall. It lies between [0,1].\n",
    "\n",
    "F1 score is derived from F Beta Score. F Beta score is the weighted harmonic mean of precision and recall.\n",
    "\n",
    "If both False Positives (FP) and False Negatives (FN) are important then β = 1.\n",
    "If False Positive (FP) is important then β lies between o and 1.\n",
    "If False Negative (FN) is important then β > 1.\n",
    "Now let us look into the code to calculate the f1 score for our cancer classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_Score: 0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'f1_Score: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd275371c28e8315e03adf8b037f110cb1b8ea1df403d06bca1b2c33c8fa2b88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
